---
title: "Data Scraping from NJDG and Some Data Clean-Up"
author: "Jeevannavar"
date: "14/02/2021"
output: html_document
---

# R Setup

Loading required packages
```{r setup}
library(tidyverse)
library(rvest)
library(xml2)
library(sf)
library(data.table)
library(foreach)
```

Custom function for transposing a data.frame
```{r}
transpose_df <- function(df) {
  t_df <- data.table::transpose(df)
  colnames(t_df) <- rownames(df)
  rownames(t_df) <- colnames(df)
  t_df <- t_df %>%
    tibble::rownames_to_column(.data = .) %>%
    tibble::as_tibble(.)
  return(t_df)
}
```

# Data scraping from the National Judicial Data Grid

## Reading the state and district codes

I manually curated these state and district codes' csv files, with some use of that small python script that's also included in this folder.
```{r}
state_codes <- read_csv("state_codes.csv")
district_codes <- read_csv("district_codes.csv")
```

## Getting the type_and_particulars right

This small data.frame is useful in one of the options in the final visualization. Also the data scraping outline that I will use is illustrated here. 

First, I read the html file at the url.  
Second, I get all the html tables in the page.  
Third, I select the requisite table, convert it to a data.frame, and write it to file.
```{r}
state_url <- paste0("https://njdg.ecourts.gov.in/njdgnew/?p=main/index&state_code=", state_codes$Code[10])
webpage <- read_html(state_url)
tbls_ls <- webpage %>%
        html_nodes("table") %>%
        .[2] %>%
        html_table(fill = TRUE)
df <- tbls_ls[[1]]

types_and_particulars <- as_tibble(df[1:2], .name_repair = "unique") %>% rename("Type" = "...1")
write_csv(types_and_particulars, "type_and_particulars.csv")
rm(state_url, webpage, tbls_ls, df)
```

## Scraping state level data

Here, I am getting the html table for each state/union territory, flattening it, and then storing it in a data.frame.

I only use `%do%` for the `foreach` chunk. This runs it on a single core/thread. To use parallel processing, you can use `%dopar%`, and `doParallel` library to set up the cluster of cores.
```{r}
cases_states <- foreach(i=1:nrow(state_codes), .combine = "rbind") %do% {
          state_url <- paste0("https://njdg.ecourts.gov.in/njdgnew/?p=main/index&state_code=", state_codes$Code[i])
        webpage <- read_html(state_url)
        tbls_ls <- webpage %>%
                html_nodes("table") %>%
                .[2] %>%
                html_table(fill = TRUE)
        df <- tbls_ls[[1]]
        df <- df %>%
                select(2:5) %>%
                pivot_longer(cols=c("Civil", "Criminal", "Total"), names_to="Type", values_to="Numbers") %>%
                unite("Particulars", 1:2)
        df$Numbers <- as.numeric(str_trim(str_replace(df$Numbers, "\\(.*\\%\\)", "")))
        df <- column_to_rownames(df, var = "Particulars")
        df <- transpose_df(df)
        slice(df,1)
}
cases_states <- cbind(State=state_codes$State, cases_states[2:ncol(cases_states)])
write_csv(cases_states, "states_cases.csv")
rm(df, state_url, webpage, tbls_ls)
```

## Scraping district level data

Here, I am getting the html table for each district in each state (at least what is available in NJDG), flattening it, and then storing it in a data.frame.

I only use `%do%` for the `foreach` chunk. This runs it on a single core/thread. To use parallel processing, you can use `%dopar%`, and `doParallel` library to set up the cluster of cores.
```{r}
cases_districts <- foreach(i=1:nrow(district_codes), .combine = "rbind") %do% {
        dist_url <- paste0("https://njdg.ecourts.gov.in/njdgnew/?p=main/index&state_code=", district_codes$state_code[i], "&dist_code=", district_codes$district_code[i], "&est_code=undefined")
        webpage <- read_html(dist_url)
        tbls_ls <- webpage %>%
                html_nodes("table") %>%
                .[2] %>%
                html_table(fill = TRUE)
        df <- tbls_ls[[1]]
        df <- df %>%
                select(2:5) %>%
                pivot_longer(cols=c("Civil", "Criminal", "Total"), names_to="Type", values_to="Numbers") %>%
                unite("Particulars", 1:2)
        df$Numbers <- as.numeric(str_trim(str_replace(df$Numbers, "\\(.*\\%\\)", "")))
        df <- column_to_rownames(df, var = "Particulars")
        df <- transpose_df(df)
        slice(df,1)
}

cases_districts <- cbind(State=district_codes$State, District=district_codes$District, cases_districts[2:ncol(cases_districts)])

## Summing over duplicates
cases_districts <- stats::aggregate(cases_districts[,3:ncol(cases_districts)], 
                                    by=list(cases_districts$State, cases_districts$District), 
                                    FUN=sum) %>% 
  rename(State = Group.1, District = Group.2)

write_csv(cases_districts, "districts_cases.csv")
rm(df, state_url, webpage, tbls_ls)
```

# Data Clean-Up

## Shapefiles

The map data is in the shape-file format. The shapefile obtained from Ecri (via ArcGIS) and the NJDG data have a few discrepancies in terms of state/district names. I resolved the discrepancies manually, and saved them to csv files termed corrected_names. Here, I'm taking the raw data and correcting the names.

I am modifying the raw map data and then saving it as a new shapefile.

```{r}
raw_shapefile <- read_sf("DemoIndia-shp/Demographics_of_India.shp")
corrected_names <- read_csv("corrected_names.csv")

corrected_shapefile <- raw_shapefile %>% 
  select(statename, distname, geometry) %>%
  rename(State = statename, District = distname)

corrected_shapefile$State <- corrected_names$State
corrected_shapefile$District <- corrected_names$District

# There are a couple duplications in the districts column whose shapes are being combined here. Paste is being used because some district names are common to multiple states
corrected_shapefile <- aggregate(corrected_shapefile,
                                 by=list(paste(corrected_shapefile$District, corrected_shapefile$State)), 
                                 FUN = unique, areaWeighted=TRUE) %>% select(!Group.1)

st_write(corrected_shapefile, "Map_data/district_level_data.shp", append = FALSE)
```

## Forming state level map

The raw map data I have here corresponds to districts. I am aggregating the polygons by state names to obtain a state level map. I then save it as another new shapefile.

The two shapefiles I just wrote are the ones I will use for the final visualization.

```{r}
state_shape <- aggregate(corrected_shapefile, by=list(corrected_shapefile$State), FUN = unique, areaWeighted=TRUE) %>% select(State)

st_write(state_shape, "Map_data/state_level_data.shp", append = FALSE)
```

## Population data

The 2011 census data is avaliable in the raw_shapefile that I loaded in an earlier chunk. The population data is taken from there. While I am not accounting for the increase in the population in the last 10 years, I still think it is useful because I'm more concerned with comparisons I will do with the data than interpret individual data points.

The underlying assumption is that the population of all states/districts grew at the same rate. It's a shame it significantly didn't.
```{r}
population_districts <- tibble(State=corrected_names$State,
                               District=corrected_names$District, 
                               `Population (Total)`=raw_shapefile$tot_p, 
                               `Population (M)`=raw_shapefile$tot_m, 
                               `Population (F)`=raw_shapefile$tot_f) %>%
  mutate_if(is.numeric,coalesce,0)

population_districts <- stats::aggregate(population_districts[3:5], 
                                         by=list(population_districts$State, population_districts$District), FUN=sum) %>%
  rename(State=Group.1, District=Group.2)

population_states <- stats::aggregate(population_districts[3:5],
                                      by=list(population_districts$State), FUN=sum) %>%
  rename(State=Group.1)

write_csv(population_districts, "districts_population.csv")
write_csv(population_states, "states_population.csv")
```
